#!/usr/bin/env python3
"""
31-advanced-phishing-system OpenAIé›†æˆæ¼”ç¤ºè„šæœ¬
å±•ç¤ºOpenAIå„ç§æ¨¡å‹çš„ä½¿ç”¨å’Œé’“é±¼å†…å®¹ç”Ÿæˆèƒ½åŠ›
"""

import os
import asyncio
import json
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class OpenAIDemo:
    def __init__(self):
        self.client = None
        self.models_tested = []
        self.results = []

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if os.getenv('OPENAI_API_KEY'):
            try:
                self.client = OpenAI(
                    api_key=os.getenv('OPENAI_API_KEY'),
                    base_url=os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
                )
                print("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.client = None

    async def test_openai_connection(self):
        """æµ‹è¯•OpenAIè¿æ¥"""
        print("\nğŸ¤– æµ‹è¯•OpenAIè¿æ¥...")

        if not self.client:
            print("âŒ OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return False

        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": "Hello, can you confirm you're working?"}],
                    max_tokens=50
                )
            )

            if response and response.choices:
                print(f"âœ… OpenAIè¿æ¥æµ‹è¯•æˆåŠŸ: {response.choices[0].message.content.strip()}")
                return True
            else:
                print("âŒ OpenAIå“åº”ä¸ºç©º")
                return False

        except Exception as e:
            print(f"âŒ OpenAIè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False

    async def test_available_models(self):
        """æµ‹è¯•å¯ç”¨æ¨¡å‹"""
        print("\nğŸ“‹ æµ‹è¯•å¯ç”¨æ¨¡å‹...")

        models_to_test = [
            "gpt-4o",
            "gpt-4-turbo",
            "gpt-3.5-turbo",
            "gpt-4",
            "gpt-3.5-turbo-16k"
        ]

        for model in models_to_test:
            try:
                print(f"  æµ‹è¯• {model}...")

                response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.client.chat.completions.create(
                        model=model,
                        messages=[{"role": "user", "content": f"Respond with 'OK' if you are {model}"}],
                        max_tokens=10
                    )
                )

                if response and response.choices:
                    self.models_tested.append({
                        "model": model,
                        "status": "available",
                        "response": response.choices[0].message.content.strip()
                    })
                    print(f"    âœ… {model} å¯ç”¨")
                else:
                    print(f"    âŒ {model} æ— å“åº”")

            except Exception as e:
                print(f"    âš ï¸  {model} ä¸å¯ç”¨: {str(e)}")
                self.models_tested.append({
                    "model": model,
                    "status": "unavailable",
                    "error": str(e)
                })

    async def generate_phishing_content_demo(self):
        """é’“é±¼å†…å®¹ç”Ÿæˆæ¼”ç¤º"""
        print("\nğŸ£ é’“é±¼å†…å®¹ç”Ÿæˆæ¼”ç¤º...")

        prompts = [
            {
                "type": "ä¼ä¸šé’“é±¼",
                "prompt": """ç”Ÿæˆä¸€å°é’ˆå¯¹ä¼ä¸šITç®¡ç†å‘˜çš„é’“é±¼é‚®ä»¶ï¼Œè¦æ±‚ä»–ä»¬ç‚¹å‡»é“¾æ¥éªŒè¯ç³»ç»Ÿå®‰å…¨è®¾ç½®ã€‚

ç›®æ ‡ä¿¡æ¯:
- å§“å: å¼ ä¸‰
- èŒä½: ITç®¡ç†å‘˜
- å…¬å¸: ABCç§‘æŠ€æœ‰é™å…¬å¸

è¦æ±‚:
1. ä½¿ç”¨æ­£å¼çš„ä¼ä¸šé‚®ä»¶é£æ ¼
2. åŒ…å«åˆç†çš„ç´§æ€¥æ€§å’Œé‡è¦æ€§
3. æ·»åŠ å¯ä¿¡çš„å…¬å¸ç»†èŠ‚
4. é¿å…æ˜æ˜¾çš„é’“é±¼ç‰¹å¾
5. åŒ…å«é€‚å½“çš„é—®å€™å’Œç­¾å""",
                "model": "gpt-4o"
            },
            {
                "type": "é«˜ç®¡é’“é±¼",
                "prompt": """ç”Ÿæˆä¸€å°é’ˆå¯¹CEOçš„ç´§æ€¥é€šçŸ¥é‚®ä»¶ï¼Œè¦æ±‚ç«‹å³å¤„ç†é‡è¦äº‹åŠ¡ã€‚

ç›®æ ‡ä¿¡æ¯:
- å§“å: ææ€»
- èŒä½: CEO
- å…¬å¸: XYZé›†å›¢

è¦æ±‚:
1. ä½¿ç”¨é«˜ç®¡é‚®ä»¶çš„æ­£å¼è¯­æ°”
2. å¼ºè°ƒç´§æ€¥æ€§å’Œé‡è¦æ€§
3. åŒ…å«å…·ä½“çš„è¡ŒåŠ¨è¦æ±‚
4. ä½¿ç”¨ä¸“ä¸šçš„ä¼ä¸šæœ¯è¯­
5. æä¾›åˆç†çš„è”ç³»æ–¹å¼""",
                "model": "gpt-4-turbo"
            },
            {
                "type": "ä¸ªäººé’“é±¼",
                "prompt": """ç”Ÿæˆä¸€å°çœ‹ä¼¼æ¥è‡ªæœ‹å‹çš„ä¸ªäººé‚®ä»¶ã€‚

ç›®æ ‡ä¿¡æ¯:
- å§“å: å°æ˜
- å…³ç³»: å¤§å­¦åŒå­¦

è¦æ±‚:
1. ä½¿ç”¨äº²åˆ‡çš„ä¸ªäººè¯­æ°”
2. åŒ…å«ä¸ªäººåŒ–çš„ç»†èŠ‚
3. æåŠå…±åŒçš„ç»å†æˆ–å…´è¶£
4. è‡ªç„¶åœ°å¼•å¯¼ç‚¹å‡»é“¾æ¥
5. é¿å…å•†ä¸šåŒ–ç‰¹å¾""",
                "model": "gpt-3.5-turbo"
            }
        ]

        for prompt_data in prompts:
            print(f"\nğŸ“§ ç”Ÿæˆ {prompt_data['type']} å†…å®¹...")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {prompt_data['model']}")

            try:
                response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.client.chat.completions.create(
                        model=prompt_data['model'],
                        messages=[{"role": "user", "content": prompt_data['prompt']}],
                        max_tokens=1000,
                        temperature=0.7
                    )
                )

                if response and response.choices:
                    content = response.choices[0].message.content.strip()
                    tokens_used = response.usage.total_tokens if response.usage else "N/A"

                    print(f"âœ… ç”ŸæˆæˆåŠŸ ({tokens_used} tokens)")
                    print(f"ğŸ“ ç”Ÿæˆå†…å®¹é¢„è§ˆ:")
                    print("-" * 50)
                    # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                    preview = content[:200] + "..." if len(content) > 200 else content
                    print(preview)
                    print("-" * 50)

                    self.results.append({
                        "type": prompt_data['type'],
                        "model": prompt_data['model'],
                        "content": content,
                        "tokens_used": tokens_used,
                        "timestamp": datetime.now().isoformat()
                    })

                else:
                    print("âŒ ç”Ÿæˆå¤±è´¥: æ— å“åº”å†…å®¹")

            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")

            # çŸ­æš‚å»¶è¿Ÿé¿å…APIé™åˆ¶
            await asyncio.sleep(1)

    async def analyze_threat_demo(self):
        """å¨èƒåˆ†ææ¼”ç¤º"""
        print("\nğŸ” å¨èƒåˆ†ææ¼”ç¤º...")

        test_content = """
ç´§æ€¥å®‰å…¨é€šçŸ¥

å°Šæ•¬çš„ç”¨æˆ·ï¼š

æˆ‘ä»¬çš„ç³»ç»Ÿæ£€æµ‹åˆ°æ‚¨çš„è´¦æˆ·å­˜åœ¨å¼‚å¸¸ç™»å½•æ´»åŠ¨ã€‚ä¸ºäº†ä¿æŠ¤æ‚¨çš„è´¦æˆ·å®‰å…¨ï¼Œè¯·ç«‹å³ç‚¹å‡»ä»¥ä¸‹é“¾æ¥éªŒè¯æ‚¨çš„èº«ä»½ä¿¡æ¯ï¼š

[å®‰å…¨éªŒè¯é“¾æ¥] https://secure-bank-login.com/verify?user=john_doe

æ­¤é“¾æ¥å°†åœ¨24å°æ—¶åè¿‡æœŸã€‚å¦‚æœæ‚¨æ²¡æœ‰è¿›è¡Œä»»ä½•ç™»å½•æ“ä½œï¼Œè¯·å¿½ç•¥æ­¤é‚®ä»¶ã€‚

æ„Ÿè°¢æ‚¨çš„é…åˆï¼

å®‰å…¨å›¢é˜Ÿ
ABCé“¶è¡Œ
        """

        print("ğŸ“„ åˆ†æå†…å®¹:")
        print("-" * 50)
        print(test_content.strip())
        print("-" * 50)

        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹é‚®ä»¶å†…å®¹æ˜¯å¦ä¸ºé’“é±¼æ”»å‡»ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ£€æµ‹æŠ¥å‘Šï¼š

é‚®ä»¶å†…å®¹:
{test_content}

è¯·ä»ä»¥ä¸‹æ–¹é¢åˆ†æ:
1. é’“é±¼ç‰¹å¾è¯†åˆ«
2. é£é™©ç­‰çº§è¯„ä¼°
3. å¯ç–‘å…ƒç´ è¯†åˆ«
4. æ£€æµ‹ç½®ä¿¡åº¦
5. å»ºè®®çš„é˜²æŠ¤æªæ–½

è¯·æä¾›ä¸“ä¸šçš„å®‰å…¨åˆ†ææŠ¥å‘Šã€‚
"""

        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": analysis_prompt}],
                    max_tokens=1500,
                    temperature=0.3
                )
            )

            if response and response.choices:
                analysis = response.choices[0].message.content.strip()
                tokens_used = response.usage.total_tokens if response.usage else "N/A"

                print("âœ… å¨èƒåˆ†æå®Œæˆ")
                print("-" * 50)
                print(analysis)
                print("-" * 50)
                print(f"ğŸ“Š Tokenä½¿ç”¨: {tokens_used}")

                self.results.append({
                    "type": "threat_analysis",
                    "model": "gpt-4o",
                    "analysis": analysis,
                    "tokens_used": tokens_used,
                    "timestamp": datetime.now().isoformat()
                })

        except Exception as e:
            print(f"âŒ å¨èƒåˆ†æå¤±è´¥: {str(e)}")

    async def evasion_techniques_demo(self):
        """è§„é¿æŠ€æœ¯æ¼”ç¤º"""
        print("\nğŸ›¡ï¸ è§„é¿æŠ€æœ¯æ¼”ç¤º...")

        original_content = "ç´§æ€¥é€šçŸ¥ï¼šæ‚¨çš„è´¦æˆ·å­˜åœ¨å®‰å…¨é£é™©ï¼Œè¯·ç«‹å³ç‚¹å‡»éªŒè¯é“¾æ¥å¤„ç†ã€‚"

        print("ğŸ“ åŸå§‹å†…å®¹:")
        print(f"  {original_content}")

        evasion_prompt = f"""
è¯·å¯¹ä»¥ä¸‹é’“é±¼é‚®ä»¶å†…å®¹åº”ç”¨è§„é¿æŠ€æœ¯ï¼Œä½¿å…¶æ›´éš¾è¢«æ£€æµ‹ç³»ç»Ÿè¯†åˆ«ï¼š

åŸå§‹å†…å®¹: {original_content}

ç›®æ ‡: ä¼ä¸šå‘˜å·¥

è§„é¿è¦æ±‚:
1. é¿å…ä½¿ç”¨å¸¸è§çš„é’“é±¼å…³é”®è¯ ("ç´§æ€¥", "ç«‹å³", "éªŒè¯")
2. ä½¿ç”¨æ›´è‡ªç„¶çš„è¯­è¨€è¡¨è¾¾
3. æ·»åŠ åˆç†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
4. è°ƒæ•´å¥å­ç»“æ„å’Œé•¿åº¦
5. åŒ…å«å¯ä¿¡çš„ç»†èŠ‚

è¯·æä¾›ä¼˜åŒ–åçš„å†…å®¹å’Œä½¿ç”¨çš„è§„é¿æŠ€æœ¯è¯´æ˜ã€‚
"""

        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": evasion_prompt}],
                    max_tokens=800,
                    temperature=0.6
                )
            )

            if response and response.choices:
                optimized_content = response.choices[0].message.content.strip()
                tokens_used = response.usage.total_tokens if response.usage else "N/A"

                print("âœ… è§„é¿ä¼˜åŒ–å®Œæˆ")
                print("ğŸ“ ä¼˜åŒ–åå†…å®¹:")
                print(f"  {optimized_content}")
                print(f"ğŸ“Š Tokenä½¿ç”¨: {tokens_used}")

                self.results.append({
                    "type": "evasion_demo",
                    "original": original_content,
                    "optimized": optimized_content,
                    "tokens_used": tokens_used,
                    "timestamp": datetime.now().isoformat()
                })

        except Exception as e:
            print(f"âŒ è§„é¿æŠ€æœ¯æ¼”ç¤ºå¤±è´¥: {str(e)}")

    def save_results(self):
        """ä¿å­˜æ¼”ç¤ºç»“æœ"""
        if self.results:
            result_file = f"openai_demo_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "demo_timestamp": datetime.now().isoformat(),
                    "openai_config": {
                        "api_key_configured": bool(os.getenv('OPENAI_API_KEY')),
                        "base_url": os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1'),
                        "models_tested": len(self.models_tested)
                    },
                    "models_status": self.models_tested,
                    "demo_results": self.results
                }, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ’¾ æ¼”ç¤ºç»“æœå·²ä¿å­˜è‡³: {result_file}")

    async def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸ­" + "="*60)
        print("        SEED 31é¡¹ç›® OpenAIé›†æˆæ¼”ç¤º")
        print("ğŸ­" + "="*60)
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        # 1. æµ‹è¯•è¿æ¥
        connection_ok = await self.test_openai_connection()
        if not connection_ok:
            print("âŒ OpenAIè¿æ¥å¤±è´¥ï¼Œæ¼”ç¤ºç»ˆæ­¢")
            return

        # 2. æµ‹è¯•å¯ç”¨æ¨¡å‹
        await self.test_available_models()

        # 3. é’“é±¼å†…å®¹ç”Ÿæˆæ¼”ç¤º
        await self.generate_phishing_content_demo()

        # 4. å¨èƒåˆ†ææ¼”ç¤º
        await self.analyze_threat_demo()

        # 5. è§„é¿æŠ€æœ¯æ¼”ç¤º
        await self.evasion_techniques_demo()

        # ä¿å­˜ç»“æœ
        self.save_results()

        print("\nğŸ­" + "="*60)
        print("                 æ¼”ç¤ºå®Œæˆ")
        print("ğŸ­" + "="*60)
        print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“Š æˆåŠŸæ¼”ç¤ºé¡¹ç›®: {len(self.results)}")
        print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹æ•°é‡: {len(self.models_tested)}")

        print("\nğŸ¯ æ¼”ç¤ºç»“æœæ€»ç»“:")
        for result in self.results:
            print(f"  âœ… {result['type']} - {result.get('model', 'N/A')} ({result.get('tokens_used', 'N/A')} tokens)")

        print("\nğŸš€ OpenAIé›†æˆçŠ¶æ€:")
        available_models = [m for m in self.models_tested if m['status'] == 'available']
        print(f"  ğŸ“‹ å¯ç”¨æ¨¡å‹: {len(available_models)}ä¸ª")
        for model in available_models:
            print(f"    â€¢ {model['model']}")

        print("\nğŸ’¡ å»ºè®®:")
        print("  â€¢ åœ¨Webç•Œé¢ä¸­ä½¿ç”¨OpenAIæ§åˆ¶å°è¿›è¡Œäº¤äº’å¼æ¼”ç¤º")
        print("  â€¢ è®¿é—® http://localhost:5003/openai_console ä½“éªŒå®Œæ•´åŠŸèƒ½")
        print("  â€¢ å¯ä»¥é…ç½®ä¸åŒçš„æ¨¡å‹å‚æ•°æ¥ä¼˜åŒ–ç”Ÿæˆæ•ˆæœ")

async def main():
    """ä¸»å‡½æ•°"""
    demo = OpenAIDemo()
    await demo.run_complete_demo()

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨OpenAIé›†æˆæ¼”ç¤º...")
    print("âš ï¸  è¯·ç¡®ä¿å·²æ­£ç¡®é…ç½®OPENAI_API_KEYå’ŒOPENAI_BASE_URLç¯å¢ƒå˜é‡")

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
